# Using OpenCode Session Data with DSPy

## Overview

This guide shows how to use the training data generated by the enhanced session logger plugin with DSPy for prompt optimization and agent improvement.

## Data Collection

### 1. Generate Training Data

Use OpenCode normally to complete coding tasks. The plugin automatically logs successful sessions to `.opencode-logs/dspy-*.json`.

**Recommended: Collect 50-100 successful examples before training**

```bash
# Check how many successful examples you have
ls .opencode-logs/dspy-*.json | wc -l

# Preview an example
cat .opencode-logs/dspy-ses_*.json | jq '.outcome.success'
```

### 2. Filter for Quality

Only successful sessions are saved, but you may want additional filtering:

```python
import json
from pathlib import Path

def load_training_data(logs_dir='.opencode-logs'):
    """Load all DSPy training examples."""
    examples = []
    
    for json_file in Path(logs_dir).glob('dspy-*.json'):
        with open(json_file) as f:
            data = json.load(f)
            
            # Only use successful sessions
            if data['outcome']['success']:
                examples.extend(data['examples'])
    
    return examples

# Load all examples
all_examples = load_training_data()
print(f"Loaded {len(all_examples)} training examples")

# Filter by quality metrics
high_quality = [
    ex for ex in all_examples
    if ex['outcome']['evaluation']['correctness'] >= 1.0 and
       ex['outcome']['evaluation']['efficiency'] > 0.7
]

print(f"High quality examples: {len(high_quality)}")
```

## DSPy Integration

### Basic Example

```python
import dspy
from dspy import Example, Signature, ChainOfThought
from pathlib import Path
import json

# Configure DSPy with your LLM
lm = dspy.OpenAI(model='gpt-4', api_key='your-key')
dspy.settings.configure(lm=lm)

# Define your signature (what the agent should do)
class CodingTask(Signature):
    """Solve a coding task by analyzing context and taking actions."""
    
    task = dspy.InputField(desc="The coding task to complete")
    context = dspy.InputField(desc="Project context including files, LSP diagnostics, git status")
    actions = dspy.OutputField(desc="List of tools to execute with arguments")
    response = dspy.OutputField(desc="Explanation of what was done")

# Load training data
def load_dspy_examples(logs_dir='.opencode-logs'):
    """Convert session logs to DSPy examples."""
    examples = []
    
    for json_file in Path(logs_dir).glob('dspy-*.json'):
        with open(json_file) as f:
            data = json.load(f)
            
            if not data['outcome']['success']:
                continue
            
            for ex in data['examples']:
                # Create DSPy Example
                example = Example(
                    task=ex['input']['task'],
                    context=json.dumps(ex['input']['context']),
                    actions=json.dumps(ex['actions']),
                    response=ex['output']['response']
                ).with_inputs('task', 'context')
                
                examples.append(example)
    
    return examples

# Load examples
train_examples = load_dspy_examples()
print(f"Loaded {len(train_examples)} examples")

# Split into train/validation
split = int(len(train_examples) * 0.8)
train = train_examples[:split]
val = train_examples[split:]

# Create a ChainOfThought module
class CodingAgent(dspy.Module):
    def __init__(self):
        super().__init__()
        self.generate_solution = ChainOfThought(CodingTask)
    
    def forward(self, task, context):
        return self.generate_solution(task=task, context=context)

# Initialize agent
agent = CodingAgent()

# Test before optimization
test_task = "Fix the TypeScript compilation error"
test_context = json.dumps({
    "projectType": "typescript",
    "lspDiagnostics": {
        "errors": [{"file": "index.ts", "line": 10, "message": "Type error"}]
    }
})

print("Before optimization:")
result = agent(task=test_task, context=test_context)
print(result.response)
```

### Optimization with BootstrapFewShot

```python
from dspy.teleprompt import BootstrapFewShot

# Define evaluation metric
def evaluate_solution(example, pred, trace=None):
    """Evaluate if the agent's solution is good."""
    
    # Check if response is substantial
    if len(pred.response) < 50:
        return False
    
    # Check if actions were specified
    try:
        actions = json.loads(pred.actions)
        if len(actions) == 0:
            return False
    except:
        return False
    
    return True

# Configure optimizer
optimizer = BootstrapFewShot(
    metric=evaluate_solution,
    max_bootstrapped_demos=8,  # Use up to 8 examples as few-shot
    max_labeled_demos=4         # Include 4 in the prompt
)

# Optimize the agent
optimized_agent = optimizer.compile(
    agent,
    trainset=train
)

# Test optimized version
print("\nAfter optimization:")
result = optimized_agent(task=test_task, context=test_context)
print(result.response)

# Evaluate on validation set
correct = 0
for ex in val[:10]:  # Test on first 10 validation examples
    pred = optimized_agent(task=ex.task, context=ex.context)
    if evaluate_solution(ex, pred):
        correct += 1

print(f"\nValidation accuracy: {correct}/10 = {correct*10}%")
```

### Advanced: Using Tool Execution Patterns

```python
from collections import Counter

# Analyze tool usage patterns from training data
def analyze_tool_patterns(examples):
    """Find common tool execution patterns."""
    
    tool_sequences = []
    tool_counts = Counter()
    
    for ex in examples:
        # Extract tool sequence
        tools = [action['tool'] for action in ex['actions']]
        tool_sequences.append(tuple(tools))
        tool_counts.update(tools)
    
    print("Most common tools:")
    for tool, count in tool_counts.most_common(10):
        print(f"  {tool}: {count}")
    
    print("\nMost common tool sequences:")
    seq_counts = Counter(tool_sequences)
    for seq, count in seq_counts.most_common(5):
        print(f"  {' -> '.join(seq)}: {count}")

# Load raw examples (not DSPy format yet)
raw_examples = []
for json_file in Path('.opencode-logs').glob('dspy-*.json'):
    with open(json_file) as f:
        data = json.load(f)
        if data['outcome']['success']:
            raw_examples.extend(data['examples'])

analyze_tool_patterns(raw_examples)
```

### Custom Signature with Tool Planning

```python
class CodingTaskWithPlanning(Signature):
    """Plan and execute a coding task step by step."""
    
    task = dspy.InputField(desc="The coding task")
    context = dspy.InputField(desc="Project context (files, errors, git status)")
    
    # Encourage reasoning about what tools to use
    plan = dspy.OutputField(desc="Step-by-step plan with tool choices")
    tools = dspy.OutputField(desc="Ordered list of tools to execute: read, edit, write, bash")
    response = dspy.OutputField(desc="Final explanation")

class PlanningAgent(dspy.Module):
    def __init__(self):
        super().__init__()
        self.planner = ChainOfThought(CodingTaskWithPlanning)
    
    def forward(self, task, context):
        return self.planner(task=task, context=context)

# Convert examples to include planning
def examples_with_planning():
    """Add tool planning to examples."""
    examples = []
    
    for json_file in Path('.opencode-logs').glob('dspy-*.json'):
        with open(json_file) as f:
            data = json.load(f)
            
            if not data['outcome']['success']:
                continue
            
            for ex in data['examples']:
                # Extract tool sequence as "plan"
                tools = [a['tool'] for a in ex['actions']]
                plan = f"1. {tools[0]}" if tools else "No tools needed"
                for i, tool in enumerate(tools[1:], 2):
                    plan += f"\n{i}. {tool}"
                
                example = Example(
                    task=ex['input']['task'],
                    context=json.dumps(ex['input']['context']),
                    plan=plan,
                    tools=json.dumps(tools),
                    response=ex['output']['response']
                ).with_inputs('task', 'context')
                
                examples.append(example)
    
    return examples

# Train planning agent
planning_examples = examples_with_planning()
planning_agent = PlanningAgent()

planning_optimizer = BootstrapFewShot(
    metric=evaluate_solution,
    max_bootstrapped_demos=6
)

optimized_planning = planning_optimizer.compile(
    planning_agent,
    trainset=planning_examples[:int(len(planning_examples)*0.8)]
)
```

## Evaluation Metrics

### Success Rate

```python
def compute_success_rate(agent, test_examples):
    """Compute how often the agent produces valid solutions."""
    
    successes = 0
    total = len(test_examples)
    
    for ex in test_examples:
        try:
            pred = agent(task=ex.task, context=ex.context)
            
            # Check if response is valid
            if (len(pred.response) > 50 and 
                'Error' not in pred.response and
                len(json.loads(pred.actions)) > 0):
                successes += 1
        except Exception as e:
            print(f"Failed on example: {e}")
            continue
    
    return successes / total if total > 0 else 0

success_rate = compute_success_rate(optimized_agent, val)
print(f"Success rate: {success_rate:.1%}")
```

### Efficiency Metrics

```python
def compute_efficiency(examples):
    """Analyze efficiency of agent solutions."""
    
    metrics = {
        'avg_tools': 0,
        'avg_time': 0,
        'avg_files_modified': 0,
        'avg_tokens': 0
    }
    
    for ex in examples:
        metrics['avg_tools'] += len(ex['actions'])
        metrics['avg_time'] += ex['outcome']['metrics']['timeToCompletion']
        metrics['avg_files_modified'] += ex['outcome']['metrics']['filesModified']
        metrics['avg_tokens'] += (
            ex['outcome']['metrics']['tokenCost']['input'] +
            ex['outcome']['metrics']['tokenCost']['output']
        )
    
    n = len(examples)
    return {k: v/n for k, v in metrics.items()}

# Compare efficiency before/after optimization
print("Efficiency metrics:")
print(compute_efficiency(raw_examples))
```

### Quality Scoring

```python
def quality_score(example, prediction):
    """Comprehensive quality score (0-1)."""
    
    scores = []
    
    # 1. Response completeness
    if len(prediction.response) > 100:
        scores.append(1.0)
    elif len(prediction.response) > 50:
        scores.append(0.5)
    else:
        scores.append(0.0)
    
    # 2. Tool usage appropriateness
    try:
        tools = json.loads(prediction.actions)
        if 1 <= len(tools) <= 5:
            scores.append(1.0)
        elif len(tools) > 5:
            scores.append(0.5)
        else:
            scores.append(0.0)
    except:
        scores.append(0.0)
    
    # 3. Error handling
    if 'error' in prediction.response.lower():
        scores.append(0.0)
    else:
        scores.append(1.0)
    
    return sum(scores) / len(scores)

# Evaluate quality
quality_scores = []
for ex in val[:20]:
    pred = optimized_agent(task=ex.task, context=ex.context)
    score = quality_score(ex, pred)
    quality_scores.append(score)

print(f"Average quality score: {sum(quality_scores)/len(quality_scores):.2f}")
```

## Saving Optimized Agents

```python
# Save optimized agent
optimized_agent.save('optimized_coding_agent.json')

# Load later
loaded_agent = CodingAgent()
loaded_agent.load('optimized_coding_agent.json')
```

## Best Practices

### 1. Data Collection
- Collect at least 50-100 successful examples
- Include diverse tasks (bug fixes, features, refactoring)
- Ensure examples represent real usage patterns

### 2. Filtering
- Only use `outcome.success === true` examples
- Filter by quality metrics (correctness â‰¥ 1.0, efficiency > 0.7)
- Remove outliers (very long or very short sessions)

### 3. Training
- Split data 80/20 train/validation
- Use cross-validation for small datasets
- Monitor overfitting on validation set

### 4. Evaluation
- Test on held-out data
- Use multiple metrics (success rate, efficiency, quality)
- Compare against baseline (unoptimized agent)

### 5. Iteration
- Continuously collect new examples
- Retrain periodically with updated data
- Monitor performance in production

## Example Workflow

```python
# 1. Collect data (happens automatically in OpenCode)
# Use OpenCode for a week, collect 100+ examples

# 2. Load and filter
examples = load_training_data()
high_quality = [ex for ex in examples if ex['outcome']['evaluation']['correctness'] >= 1.0]

# 3. Split data
split = int(len(high_quality) * 0.8)
train = high_quality[:split]
val = high_quality[split:]

# 4. Create agent
agent = CodingAgent()

# 5. Optimize
optimizer = BootstrapFewShot(metric=evaluate_solution, max_bootstrapped_demos=8)
optimized = optimizer.compile(agent, trainset=train)

# 6. Evaluate
success = compute_success_rate(optimized, val)
print(f"Success rate: {success:.1%}")

# 7. Save
optimized.save('production_agent.json')

# 8. Deploy
# Use optimized agent in your application
```

## Troubleshooting

### Not Enough Training Data
- Complete more tasks in OpenCode
- Lower quality thresholds temporarily
- Use data augmentation techniques

### Low Success Rate
- Check if examples are too diverse
- Simplify the signature
- Use more bootstrapped demos
- Review failed examples for patterns

### Overfitting
- Increase validation set size
- Use regularization in optimizer
- Collect more diverse training data
- Reduce max_bootstrapped_demos

## Resources

- [DSPy Documentation](https://dspy-docs.vercel.app/)
- [DSPy GitHub](https://github.com/stanfordnlp/dspy)
- [OpenCode Plugin Docs](https://opencode.ai/docs)

## Next Steps

1. Collect 50-100 examples by using OpenCode normally
2. Run the basic example to verify data format
3. Experiment with different signatures
4. Optimize and evaluate
5. Deploy optimized agent to production
